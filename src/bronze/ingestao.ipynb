{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2b6516-6aef-4c8e-9810-f2dfa25aa1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üì• Pipeline de Ingest√£o CDC - Upcell\n",
    "\n",
    "Este notebook implementa o pipeline de ingest√£o de dados CDC (Change Data Capture) do S3 para o Bronze no Databricks.\n",
    "\n",
    "## üéØ Objetivo\n",
    "- **Full-load**: Carga inicial completa das tabelas\n",
    "- **CDC**: Ingest√£o incremental com opera√ß√µes Insert, Update e Delete\n",
    "- **Delta Lake**: Merge at√¥mico na camada Bronze\n",
    "\n",
    "## üìã Requisitos\n",
    "- Tabelas no S3: `s3://meudatalake-raw/upcell/`\n",
    "- Cat√°logo: `bronze.upcell`\n",
    "- Coluna de controle: `DtAtualizacao` (presente em todos os arquivos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96210ef1-6082-4adc-997a-06fd79fc047b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1Ô∏è‚É£ Importa√ß√µes e Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47c0ecf-3b66-4795-8540-b1ccb1cab09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import delta\n",
    "def table_exists(catalog, database, table):\n",
    "    count = (spark.sql(f\"SHOW TABLES IN `{catalog}`.`{database}`\")\n",
    "               .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "               .count())\n",
    "    return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec0451d-f1c7-458e-b4c1-5a5c577dafe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"bronze\"\n",
    "schema = \"upcell\"\n",
    "tablename = dbutils.widgets.get(\"tablename\")\n",
    "id_field = dbutils.widgets.get(\"id_field\")\n",
    "timefield = dbutils.widgets.get(\"timefield\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb589cf4-1c52-4041-b104-01109d2419f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2Ô∏è‚É£ Configura√ß√£o da Tabela\n",
    "\n",
    "Defina a tabela que ser√° processada e os campos de controle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695b4c24-a460-4a71-8355-5bd71401d147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carga inicial: Cria tabela Delta a partir do full-load\n",
    "if not table_exists(catalog, schema, tablename):\n",
    "    print(f\"‚ö†Ô∏è  Tabela {catalog}.{schema}.{tablename} N√ÉO existe. Criando a partir do full-load...\")\n",
    "\n",
    "    # L√™ full-load (j√° tem DtAtualizacao!)\n",
    "    df_full = spark.read.format(\"parquet\").load(f\"/Volumes/raw/upcell/full-load/{tablename}\")\n",
    "    \n",
    "    print(f\"üìä Total de registros no full-load: {df_full.count():,}\")\n",
    "\n",
    "    # Cria tabela Delta\n",
    "    (df_full.coalesce(1)\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(f\"{catalog}.{schema}.{tablename}\"))\n",
    "    \n",
    "    print(f\"‚úÖ Tabela {catalog}.{schema}.{tablename} criada com sucesso!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚úÖ Tabela {catalog}.{schema}.{tablename} j√° existe. Pular para o CDC merge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbad948-63ab-4244-8c07-231342112e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3Ô∏è‚É£ Full-Load (Carga Inicial)\n",
    "\n",
    "Se a tabela n√£o existe, cria a partir dos dados de full-load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98bf0438-167c-4336-9de0-7636723e3a7e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759523012519}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Processa CDC: Filtra apenas arquivos novos e deduplica\n",
    "print(\"üì• Carregando dados CDC...\")\n",
    "\n",
    "# 1. Busca a √∫ltima atualiza√ß√£o j√° processada na tabela Bronze\n",
    "last_processed = spark.sql(f\"\"\"\n",
    "    SELECT COALESCE(MAX(DtAtualizacao), '1900-01-01') as last_dt\n",
    "    FROM {catalog}.{schema}.{tablename}\n",
    "\"\"\").collect()[0]['last_dt']\n",
    "\n",
    "print(f\"? √öltima atualiza√ß√£o processada: {last_processed}\")\n",
    "\n",
    "# 2. L√™ TODOS os arquivos CDC (por enquanto)\n",
    "(spark.read\n",
    "    .format(\"parquet\")\n",
    "    .load(f\"/Volumes/raw/upcell/cdc/{tablename}\")\n",
    "    .createOrReplaceTempView(f\"view_{tablename}\"))\n",
    "\n",
    "# 3. Filtra apenas registros NOVOS (DtAtualizacao > √∫ltima processada)\n",
    "query_filter = f\"\"\"\n",
    "    SELECT *  \n",
    "    FROM view_{tablename}\n",
    "    WHERE DtAtualizacao > '{last_processed}'\n",
    "\"\"\"\n",
    "\n",
    "df_cdc_new = spark.sql(query_filter)\n",
    "total_new_records = df_cdc_new.count()\n",
    "\n",
    "print(f\"üìä Total de registros CDC NOVOS: {total_new_records:,}\")\n",
    "\n",
    "if total_new_records == 0:\n",
    "    print(\"‚ö†Ô∏è  Nenhum registro CDC novo encontrado. Pulando merge.\")\n",
    "    df_cdc_unique = df_cdc_new  # DataFrame vazio\n",
    "else:\n",
    "    # 4. Deduplica: Pega apenas o √∫ltimo registro de cada chave (nos dados NOVOS)\n",
    "    query_dedupe = f\"\"\"\n",
    "        SELECT *  \n",
    "        FROM view_{tablename}\n",
    "        WHERE DtAtualizacao > '{last_processed}'\n",
    "        QUALIFY ROW_NUMBER() OVER(PARTITION BY {id_field} ORDER BY {timefield} DESC) = 1\n",
    "    \"\"\"\n",
    "    \n",
    "    df_cdc_unique = spark.sql(query_dedupe)\n",
    "    \n",
    "    print(f\"üìä Total de registros CDC √∫nicos (ap√≥s deduplica√ß√£o): {df_cdc_unique.count():,}\")\n",
    "    print(f\"üìã Opera√ß√µes no CDC:\")\n",
    "    df_cdc_unique.groupBy(\"op\").count().display()\n",
    "    \n",
    "    print(\"\\nüîç Sample de registros CDC:\")\n",
    "    df_cdc_unique.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f5da95f-6268-4ec0-9593-c33bc0e6ed54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4Ô∏è‚É£ Processamento CDC\n",
    "\n",
    "Carrega arquivos CDC, deduplica e prepara para o merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993dd37d-2020-4199-9953-755f19cc570c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze = delta.DeltaTable.forName(spark, f\"{catalog}.{schema}.{tablename}\")\n",
    "bronze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83447613-0f6a-4873-bbc4-e930cbc3448c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# üìä ANTES DO MERGE: Captura estat√≠sticas atuais\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä ESTAT√çSTICAS ANTES DO MERGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Contagem total antes\n",
    "count_before = spark.sql(f\"SELECT COUNT(*) as total FROM {catalog}.{schema}.{tablename}\").collect()[0]['total']\n",
    "print(f\"\\n‚úÖ Total de registros ANTES: {count_before:,}\")\n",
    "\n",
    "# Detalhes da tabela antes\n",
    "details_before = spark.sql(f\"DESCRIBE DETAIL {catalog}.{schema}.{tablename}\").select(\"numFiles\", \"sizeInBytes\").collect()[0]\n",
    "print(f\"üìÅ Arquivos: {details_before['numFiles']}\")\n",
    "print(f\"üíæ Tamanho: {details_before['sizeInBytes']:,} bytes ({details_before['sizeInBytes'] / (1024*1024):.2f} MB)\")\n",
    "\n",
    "# √öltima atualiza√ß√£o antes\n",
    "last_update_before = spark.sql(f\"\"\"\n",
    "    SELECT MAX(DtAtualizacao) as ultima_atualizacao \n",
    "    FROM {catalog}.{schema}.{tablename}\n",
    "\"\"\").collect()[0]['ultima_atualizacao']\n",
    "print(f\"üïê √öltima atualiza√ß√£o: {last_update_before}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14128293-5e45-44b0-aab8-297b23d494f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge CDC na tabela Delta Bronze\n",
    "print(\"üîÑ Executando merge CDC na tabela Bronze...\")\n",
    "\n",
    "# Verifica se h√° dados novos para processar\n",
    "if df_cdc_unique.count() == 0:\n",
    "    print(\"‚è≠Ô∏è  Nenhum dado CDC novo. Merge n√£o executado.\")\n",
    "    \n",
    "    # Define vari√°veis para compara√ß√£o (sem mudan√ßas)\n",
    "    count_after = count_before\n",
    "    details_after = details_before\n",
    "    last_update_after = last_update_before\n",
    "    \n",
    "else:\n",
    "    bronze = delta.DeltaTable.forName(spark, f\"{catalog}.{schema}.{tablename}\")\n",
    "    \n",
    "    (bronze.alias(\"b\") \n",
    "      .merge(df_cdc_unique.alias(\"d\"), f\"b.{id_field} = d.{id_field}\") \n",
    "      .whenMatchedDelete(condition = \"d.op = 'D'\")           # Delete se op = 'D'\n",
    "      .whenMatchedUpdateAll(condition = \"d.op = 'U'\")        # Update se op = 'U'\n",
    "      .whenNotMatchedInsertAll(condition = \"d.op = 'I'\")     # Insert se op = 'I'\n",
    "      .execute()\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Merge CDC executado com sucesso!\")\n",
    "    \n",
    "    # üìä DEPOIS DO MERGE: Captura estat√≠sticas atualizadas\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä ESTAT√çSTICAS DEPOIS DO MERGE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Contagem total depois\n",
    "    count_after = spark.sql(f\"SELECT COUNT(*) as total FROM {catalog}.{schema}.{tablename}\").collect()[0]['total']\n",
    "    print(f\"\\n‚úÖ Total de registros DEPOIS: {count_after:,}\")\n",
    "    \n",
    "    # Detalhes da tabela depois\n",
    "    details_after = spark.sql(f\"DESCRIBE DETAIL {catalog}.{schema}.{tablename}\").select(\"numFiles\", \"sizeInBytes\").collect()[0]\n",
    "    print(f\"üìÅ Arquivos: {details_after['numFiles']}\")\n",
    "    print(f\"üíæ Tamanho: {details_after['sizeInBytes']:,} bytes ({details_after['sizeInBytes'] / (1024*1024):.2f} MB)\")\n",
    "    \n",
    "    # √öltima atualiza√ß√£o depois\n",
    "    last_update_after = spark.sql(f\"\"\"\n",
    "        SELECT MAX(DtAtualizacao) as ultima_atualizacao \n",
    "        FROM {catalog}.{schema}.{tablename}\n",
    "    \"\"\").collect()[0]['ultima_atualizacao']\n",
    "    print(f\"üïê √öltima atualiza√ß√£o: {last_update_after}\")\n",
    "\n",
    "# üîÑ COMPARA√á√ÉO: Calcula diferen√ßas\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ COMPARA√á√ÉO: ANTES vs DEPOIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "diff_records = count_after - count_before\n",
    "diff_size = details_after['sizeInBytes'] - details_before['sizeInBytes']\n",
    "diff_files = details_after['numFiles'] - details_before['numFiles']\n",
    "\n",
    "print(f\"\\nüìä Diferen√ßa de registros: {diff_records:+,} ({'+' if diff_records >= 0 else ''}{(diff_records/count_before*100 if count_before > 0 else 0):.2f}%)\")\n",
    "print(f\"üíæ Diferen√ßa de tamanho: {diff_size:+,} bytes ({diff_size / (1024*1024):+.2f} MB)\")\n",
    "print(f\"üìÅ Diferen√ßa de arquivos: {diff_files:+}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69947e7-7b1e-4271-8f50-95f3d18d483b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5Ô∏è‚É£ Merge CDC na Tabela Delta\n",
    "\n",
    "Aplica as opera√ß√µes de Insert, Update e Delete na camada Bronze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b72764-f3c5-448c-94dd-4de3be8fdcc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Valida√ß√£o 1: Contagem total de registros\n",
    "total = spark.sql(f\"SELECT COUNT(*) as total FROM {catalog}.{schema}.{tablename}\").collect()[0]['total']\n",
    "print(f\"üìä Total de registros na tabela Bronze: {total:,}\")\n",
    "\n",
    "# Valida√ß√£o 2: Verificar se DtAtualizacao est√° presente\n",
    "sample = spark.sql(f\"SELECT * FROM {catalog}.{schema}.{tablename} LIMIT 5\")\n",
    "print(f\"\\n‚úÖ Colunas da tabela: {sample.columns}\")\n",
    "sample.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d143d8c9-db0a-4553-9e95-a5276761c8f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Valida√ß√£o 3: Verificar hist√≥rico de vers√µes Delta\n",
    "print(\"üìú Hist√≥rico de vers√µes da tabela Delta:\\n\")\n",
    "spark.sql(f\"DESCRIBE HISTORY {catalog}.{schema}.{tablename}\").select(\n",
    "    \"version\", \n",
    "    \"timestamp\", \n",
    "    \"operation\", \n",
    "    \"operationMetrics\"\n",
    ").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5195627179372982,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
