{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2b6516-6aef-4c8e-9810-f2dfa25aa1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pipeline de Ingestão CDC - Upcell\n",
    "\n",
    "Este notebook implementa o pipeline de ingestão de dados CDC (Change Data Capture) do S3 para o Bronze no Databricks.\n",
    "\n",
    "## Objetivo\n",
    "- Full-load: Carga inicial completa das tabelas\n",
    "- CDC: Ingestão incremental com operações Insert, Update e Delete\n",
    "- Delta Lake: Merge atômico na camada Bronze\n",
    "\n",
    "## Requisitos\n",
    "- Tabelas no S3: `s3://meudatalake-raw/upcell/`\n",
    "- Catálogo: `bronze.upcell`\n",
    "- Coluna de controle: `DtAtualizacao` (presente em todos os arquivos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96210ef1-6082-4adc-997a-06fd79fc047b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1️⃣ Importações e Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47c0ecf-3b66-4795-8540-b1ccb1cab09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import delta\n",
    "def table_exists(catalog, database, table):\n",
    "    count = (spark.sql(f\"SHOW TABLES IN `{catalog}`.`{database}`\")\n",
    "               .filter(f\"database = '{database}' AND tableName = '{table}'\")\n",
    "               .count())\n",
    "    return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec0451d-f1c7-458e-b4c1-5a5c577dafe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"bronze\"\n",
    "schema = \"upcell\"\n",
    "tablename = dbutils.widgets.get(\"tablename\")\n",
    "id_field = dbutils.widgets.get(\"id_field\")\n",
    "timefield = dbutils.widgets.get(\"timefield\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb589cf4-1c52-4041-b104-01109d2419f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Importações e Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695b4c24-a460-4a71-8355-5bd71401d147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carga inicial: Cria tabela Delta a partir do full-load\n",
    "if not table_exists(catalog, schema, tablename):\n",
    "    print(f\"Tabela {catalog}.{schema}.{tablename} NÃO existe. Criando a partir do full-load...\")\n",
    "\n",
    "    # Lê full-load (já tem DtAtualizacao!)\n",
    "    df_full = spark.read.format(\"parquet\").load(f\"/Volumes/raw/upcell/full-load/{tablename}\")\n",
    "    print(f\"Total de registros no full-load: {df_full.count():,}\")\n",
    "\n",
    "    # Cria tabela Delta\n",
    "    (df_full.coalesce(1)\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(f\"{catalog}.{schema}.{tablename}\"))\n",
    "    print(f\"Tabela {catalog}.{schema}.{tablename} criada com sucesso!\")\n",
    "else:\n",
    "    print(f\"Tabela {catalog}.{schema}.{tablename} já existe. Pular para o CDC merge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbad948-63ab-4244-8c07-231342112e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Full-Load (Carga Inicial)\n",
    "\n",
    "Se a tabela não existe, cria a partir dos dados de full-load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98bf0438-167c-4336-9de0-7636723e3a7e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759523012519}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Processa CDC: Filtra apenas arquivos novos e deduplica\n",
    "# 1. Busca a última atualização já processada na tabela Bronze\n",
    "last_processed = spark.sql(f\"\"\"\n",
    "    SELECT COALESCE(MAX(DtAtualizacao), '1900-01-01') as last_dt\n",
    "    FROM {catalog}.{schema}.{tablename}\n",
    "\"\"\").collect()[0]['last_dt']\n",
    "\n",
    "# 2. Lê TODOS os arquivos CDC (por enquanto)\n",
    "(spark.read\n",
    "    .format(\"parquet\")\n",
    "    .load(f\"/Volumes/raw/upcell/cdc/{tablename}\")\n",
    "    .createOrReplaceTempView(f\"view_{tablename}\"))\n",
    "\n",
    "# 3. Filtra apenas registros NOVOS (DtAtualizacao > última processada)\n",
    "query_filter = f\"\"\"\n",
    "    SELECT *  \n",
    "    FROM view_{tablename}\n",
    "    WHERE DtAtualizacao > '{last_processed}'\n",
    "\"\"\"\n",
    "\n",
    "df_cdc_new = spark.sql(query_filter)\n",
    "total_new_records = df_cdc_new.count()\n",
    "\n",
    "if total_new_records == 0:\n",
    "    df_cdc_unique = df_cdc_new  # DataFrame vazio\n",
    "else:\n",
    "    # 4. Deduplica: Pega apenas o último registro de cada chave (nos dados NOVOS)\n",
    "    query_dedupe = f\"\"\"\n",
    "        SELECT *  \n",
    "        FROM view_{tablename}\n",
    "        WHERE DtAtualizacao > '{last_processed}'\n",
    "        QUALIFY ROW_NUMBER() OVER(PARTITION BY {id_field} ORDER BY {timefield} DESC) = 1\n",
    "    \"\"\"\n",
    "    df_cdc_unique = spark.sql(query_dedupe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f5da95f-6268-4ec0-9593-c33bc0e6ed54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Processamento CDC\n",
    "\n",
    "Carrega arquivos CDC, deduplica e prepara para o merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993dd37d-2020-4199-9953-755f19cc570c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze = delta.DeltaTable.forName(spark, f\"{catalog}.{schema}.{tablename}\")\n",
    "bronze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83447613-0f6a-4873-bbc4-e930cbc3448c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Estatísticas antes do merge\n",
    "count_before = spark.sql(f\"SELECT COUNT(*) as total FROM {catalog}.{schema}.{tablename}\").collect()[0]['total']\n",
    "details_before = spark.sql(f\"DESCRIBE DETAIL {catalog}.{schema}.{tablename}\").select(\"numFiles\", \"sizeInBytes\").collect()[0]\n",
    "last_update_before = spark.sql(f\"\"\"\n",
    "    SELECT MAX(DtAtualizacao) as ultima_atualizacao \n",
    "    FROM {catalog}.{schema}.{tablename}\n",
    "\"\"\").collect()[0]['ultima_atualizacao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14128293-5e45-44b0-aab8-297b23d494f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge CDC na tabela Delta Bronze\n",
    "# Verifica se há dados novos para processar\n",
    "if df_cdc_unique.count() == 0:\n",
    "    # Define variáveis para comparação (sem mudanças)\n",
    "    count_after = count_before\n",
    "    details_after = details_before\n",
    "    last_update_after = last_update_before\n",
    "else:\n",
    "    bronze = delta.DeltaTable.forName(spark, f\"{catalog}.{schema}.{tablename}\")\n",
    "    (bronze.alias(\"b\") \n",
    "      .merge(df_cdc_unique.alias(\"d\"), f\"b.{id_field} = d.{id_field}\") \n",
    "      .whenMatchedDelete(condition = \"d.op = 'D'\")           # Delete se op = 'D'\n",
    "      .whenMatchedUpdateAll(condition = \"d.op = 'U'\")        # Update se op = 'U'\n",
    "      .whenNotMatchedInsertAll(condition = \"d.op = 'I'\")     # Insert se op = 'I'\n",
    "      .execute()\n",
    "    )\n",
    "    # Estatísticas depois do merge\n",
    "    count_after = spark.sql(f\"SELECT COUNT(*) as total FROM {catalog}.{schema}.{tablename}\").collect()[0]['total']\n",
    "    details_after = spark.sql(f\"DESCRIBE DETAIL {catalog}.{schema}.{tablename}\").select(\"numFiles\", \"sizeInBytes\").collect()[0]\n",
    "    last_update_after = spark.sql(f\"\"\"\n",
    "        SELECT MAX(DtAtualizacao) as ultima_atualizacao \n",
    "        FROM {catalog}.{schema}.{tablename}\n",
    "    \"\"\").collect()[0]['ultima_atualizacao']\n",
    "# Comparação antes vs depois\n",
    "diff_records = count_after - count_before\n",
    "diff_size = details_after['sizeInBytes'] - details_before['sizeInBytes']\n",
    "diff_files = details_after['numFiles'] - details_before['numFiles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69947e7-7b1e-4271-8f50-95f3d18d483b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Merge CDC na Tabela Delta\n",
    "\n",
    "Aplica as operações de Insert, Update e Delete na camada Bronze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b72764-f3c5-448c-94dd-4de3be8fdcc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validação 1: Contagem total de registros\n",
    "total = spark.sql(f\"SELECT COUNT(*) as total FROM {catalog}.{schema}.{tablename}\").collect()[0]['total']\n",
    "print(f\"Total de registros na tabela Bronze: {total:,}\")\n",
    "\n",
    "# Validação 2: Verificar se DtAtualizacao está presente\n",
    "sample = spark.sql(f\"SELECT * FROM {catalog}.{schema}.{tablename} LIMIT 5\")\n",
    "print(f\"Colunas da tabela: {sample.columns}\")\n",
    "sample.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d143d8c9-db0a-4553-9e95-a5276761c8f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validação 3: Verificar histórico de versões Delta\n",
    "spark.sql(f\"DESCRIBE HISTORY {catalog}.{schema}.{tablename}\").select(\n",
    "    \"version\", \n",
    "    \"timestamp\", \n",
    "    \"operation\", \n",
    "    \"operationMetrics\"\n",
    ").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5195627179372982,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
