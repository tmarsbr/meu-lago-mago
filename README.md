# Meu Lago Mago ğŸª„

## VisÃ£o Geral

Este projeto constrÃ³i um **Data Lakehouse** do zero no Databricks, seguindo as melhores prÃ¡ticas de engenharia de dados. Inspirado na playlist **"Lago do Mago"** de TÃ©o Me Why, implementamos uma arquitetura em camadas (Bronze, Silver, Gold) com integraÃ§Ã£o entre **CDC (Change Data Capture)** e **Full Load**, utilizando o ecossistema Databricks para processamento de dados em larga escala.

O foco atual estÃ¡ na **camada Bronze**, responsÃ¡vel pela ingestÃ£o e armazenamento bruto dos dados, preparando o terreno para transformaÃ§Ãµes nas camadas superiores.

## Arquitetura do Lakehouse

A arquitetura segue o modelo tradicional de Lakehouse:

- **Bronze Layer**: Dados brutos ingeridos diretamente das fontes, com mÃ­nimo processamento. Armazenamento em Delta Lake para versionamento e ACID transactions.
- **Silver Layer**: Dados limpos e transformados, prontos para anÃ¡lise.
- **Gold Layer**: Dados agregados e otimizados para consumo por aplicaÃ§Ãµes e dashboards.

Fluxo atual (atÃ© Aula 02): `clientes` â†’ `transacao` (ingestÃ£o sequencial com dependÃªncias via Lakeflow Jobs).

## Etapas ConcluÃ­das (AtÃ© Aula 02)

âœ… **ConfiguraÃ§Ã£o do Ambiente**
- Setup do workspace Databricks com Unity Catalog para governanÃ§a de dados.
- IntegraÃ§Ã£o com repositÃ³rio GitHub (`meu-lago-mago`) para versionamento de cÃ³digo.

âœ… **Estrutura de Pastas**
- OrganizaÃ§Ã£o inicial: `src/bronze/ingestao/`.
- Notebooks desenvolvidos: `bronze_clientes.ipynb` e `bronze_transacao.ipynb`.

âœ… **Desenvolvimento dos Notebooks Bronze**
- `bronze_clientes`: IngestÃ£o de dados de clientes via Full Load.
- `bronze_transacao`: IngestÃ£o de dados de transaÃ§Ãµes via CDC, com merge incremental.

âœ… **OrquestraÃ§Ã£o com Lakeflow Jobs**
- CriaÃ§Ã£o de jobs no Databricks para execuÃ§Ã£o automatizada.
- DependÃªncia sequencial: `bronze_clientes` executa antes de `bronze_transacao` para evitar conflitos de concorrÃªncia.

âœ… **Tratamento de Erros e OtimizaÃ§Ãµes**
- ResoluÃ§Ã£o de `ConcurrentAppendException` atravÃ©s de execuÃ§Ã£o sequencial.
- ImplementaÃ§Ã£o de controle transacional Delta para merges seguros.

## Pipeline de ExecuÃ§Ã£o

![ExecuÃ§Ã£o do Lakeflow Job](job%20e%20pipeline.png)

A imagem mostra a execuÃ§Ã£o do Lakeflow Job no Databricks, responsÃ¡vel pela orquestraÃ§Ã£o das tabelas da camada Bronze do projeto meu-lago-mago. O fluxo de tarefas Ã© sequencial, representando o processo de ingestÃ£o e consolidaÃ§Ã£o de dados com controle de dependÃªncia entre tabelas.

- **bronze_clientes** â†’ Primeira tarefa do pipeline, encarregada de processar e aplicar o merge CDC dos dados de clientes na camada Bronze.
- **bronze_transacao** â†’ Executada automaticamente apÃ³s a conclusÃ£o da etapa anterior, realiza o merge CDC das transaÃ§Ãµes, garantindo a consistÃªncia e integridade referencial entre entidades.

Cada bloco verde indica execuÃ§Ã£o bem-sucedida no Cluster de Tiago Silva, e a seta entre as tarefas evidencia a dependÃªncia de execuÃ§Ã£o: a segunda inicia apenas quando a primeira finaliza com sucesso. Esse mecanismo previne conflitos de escrita no Delta Lake (como o ConcurrentAppendException) e assegura que os merges ocorram em ordem controlada.

ğŸ“ˆ Essa execuÃ§Ã£o confirma a estabilidade do pipeline de ingestÃ£o e valida a estrutura transacional da camada Bronze dentro do Lakehouse.

## Tecnologias e Ferramentas

- ğŸ—ï¸ **Databricks**: Plataforma para processamento de big data e machine learning.
- ğŸï¸ **Delta Lake**: Formato de armazenamento com ACID transactions e versionamento.
- âš¡ **PySpark**: API Python para Apache Spark, usada para processamento distribuÃ­do.
- ğŸ“š **Unity Catalog**: Sistema de governanÃ§a para metadados e controle de acesso.
- ğŸ™ **GitHub**: Controle de versÃ£o e colaboraÃ§Ã£o.
- ğŸ”„ **Lakeflow Jobs**: OrquestraÃ§Ã£o de pipelines no Databricks.

## PrÃ³ximos Passos

ğŸš€ **Aula 03 em diante**: ExpansÃ£o para camadas Silver e Gold.
- TransformaÃ§Ãµes e limpeza de dados na Silver Layer.
- AgregaÃ§Ãµes e modelagem dimensional na Gold Layer.
- IntegraÃ§Ã£o avanÃ§ada de CDC e otimizaÃ§Ã£o de performance.
- ImplementaÃ§Ã£o de dashboards e relatÃ³rios finais.

## Aula 3 â€” CDC com streaming

![Stream pipeline](docs/stream.png)

Nesta aula implementamos ingestÃ£o contÃ­nua de eventos CDC (Change Data Capture) usando Structured Streaming do Spark para manter a camada Bronze atualizada em tempo quase real.

- Abordagem:
	- Ler arquivos CDC (Parquet/JSON) como stream com `spark.readStream`.
	- Usar um schema explÃ­cito capturado a partir de amostras para evitar inferÃªncia dinÃ¢mica.
	- Gravar em Delta Lake com `checkpointLocation` para garantir consistÃªncia entre execuÃ§Ãµes.
	- Aplicar merges incrementais em tabelas Bronze para refletir inserts/updates/deletes.

- Notebooks relacionados:
	- `src/bronze/ingestao.ipynb` â€” contÃ©m a implementaÃ§Ã£o do streaming (captura de schema, leitura via `spark.readStream`, checkpoint e `writeStream`).
	- `src/bronze/bronze_transacao.ipynb` â€” adaptaÃ§Ãµes para merges a partir de eventos CDC.

- Como executar (resumo):
	1. Preparar a pasta de raw CDC (ex.: `/mnt/raw/cdc/<tabela>/`) com arquivos de eventos.
	2. Ajustar variÃ¡veis de caminho e nomes de tabela no notebook.
	3. Executar a cÃ©lula que inicia o stream (`stream.start()`) no Databricks.
	4. Monitorar o progresso no UI de Streaming do Databricks e validar escrita em Delta.

- ObservaÃ§Ãµes e recomendaÃ§Ãµes:
	- Defina `checkpointLocation` em storage persistente (ADLS/S3) para recuperaÃ§Ã£o correta.
	- Ajuste `maxFilesPerTrigger` / `trigger` para balancear latÃªncia e throughput.
	- Para operaÃ§Ãµes de delete no CDC, trate tombstones ou implemente lÃ³gica de soft-delete antes do merge.
	- Use orquestraÃ§Ã£o (Lakeflow Jobs) para evitar conflitos de escrita e gerenciar dependÃªncias.

## CrÃ©ditos e ReferÃªncias

Este projeto Ã© baseado na playlist **"Lago do Mago"** de **TÃ©o Me Why**, disponÃ­vel no YouTube. Agradecimentos especiais ao criador por compartilhar conhecimentos valiosos sobre engenharia de dados no Databricks.

ğŸ”— [Playlist Lago do Mago - YouTube](https://www.youtube.com/playlist?list=PLvlkVRRKOYFTcLehYZ2Bd5hGIcLH0dJHE)

GitHub: [@TeoMeWhy](https://github.com/TeoMeWhy)

## Autores e ContribuiÃ§Ãµes

- **Tiago Silva** (@tmarsbr) - Desenvolvedor principal e implementador do projeto no Databricks.
- **Teo Calvo** (@TeoCalvo) - Criador da playlist "Lago do Mago" e fonte de inspiraÃ§Ã£o para as melhores prÃ¡ticas de engenharia de dados.

---

*Projeto desenvolvido com dedicaÃ§Ã£o para aprendizado e aplicaÃ§Ã£o prÃ¡tica de conceitos de Data Lakehouse. ğŸ§™â€â™‚ï¸*
